{"type":"data","nodes":[null,{"type":"data","data":[{"meta":1,"content":4},{"title":2,"date":3},"Graphing Wikipedia","2026-01-10","\u003Cp>\u003Ca href=\"\">\u003C/a>\u003C/p>\n\u003Cp>To begin, I should mention that I am in no way a data scientist or programmer really specializing in graphing in any way. I came up with this project after reading about the \u003Ca href=\"https://en.wikipedia.org/wiki/Wikipedia_philosophy_phenomenon\">Philosophy game\u003C/a> and finding out you can \u003Ca href=\"https://en.wikipedia.org/wiki/Wikipedia:Database_download\">download a dump of Wikipedia\u003C/a> without needing a couple of hard drives to store it all. As the title states, the goal is to graph Wikipedia (partially, you’ll see) and as with every project I do, I wanted to learn certain skills as well as getting a nice result. In this case, I wanted to learn how to use the dedicated Wikipedia dumping tools, store large amounts of data as efficiently as possible, make my program’s runtime decently quick, and display a large amount of data in a way that is aesthetically pleasing.\u003C/p>\n\u003Cp>To start this project, I first downloaded a dump of Wikipedia. Luckily, they make that as easy as possible, and you can get a \u003Ca href=\"https://meta.wikimedia.org/wiki/Data_dump_torrents#English_Wikipedia\">~25GB compressed dump from here\u003C/a>. You can either decompress it to ~120GB or leave it compressed and gradually decompress it while processing it to save space. I ended up decompressing it before working because I had the storage, and as we will discuss in much more detail later, probably need more memory in my system to keep doing these kinds of projects.\u003C/p>\n\u003Cp>With the data in hand, I was initially thinking that I could just… read it. But unfortunately, in order to get all of this data into such a small package, the Wikipedia export is in XML format, and needs to be streamed out of the file. This means you can’t exactly export only a single page, but rather that you need to flip through every page of this massive encyclopedia before you find what you need. This is where the programming starts.\u003C/p>\n\u003Cp>In order to graph all of Wikipedia, I set three ground rules:\u003C/p>\n\u003Cp>1.) I will not be making my own display system for this. This means using \u003Ca href=\"https://gephi.org/\">Gephi\u003C/a> to display everything. If I wrote this myself, the project would just end up being me making a graph display system. Again, I am not someone well versed in graph theory, I just like making cool projects. This doesn’t need to take a year, just a couple of weeks.\u003C/p>\n\u003Cp>2.) I will not be displaying everything in order to make the graph useful. Graphing Wikipedia can be done by a billion different metrics, and I could graph based on how many words pages have, or general similarities. For this project, I will only be graphing the relation of pages first links. The idea behind this is that of the Philosophy game. Essentially, the idea that all pages when clicking exclusively their first links, after enough clicks, you will end up at the page for \u003Ca href=\"https://en.wikipedia.org/wiki/Philosophy\">Philosophy\u003C/a>. Try it. Start on the page \u003Ca href=\"https://en.wikipedia.org/wiki/Banana\">Banana\u003C/a> and ignore \u003Ca href=\"https://en.wikipedia.org/wiki/Help:Disambiguation\">disambiguation\u003C/a> pages, \u003Ca href=\"https://en.wikipedia.org/wiki/Language\">languages\u003C/a>, and \u003Ca href=\"https://en.wikipedia.org/wiki/Help:IPA/Japanese\">phonetics\u003C/a>. The link train goes from \u003Ca href=\"https://en.wikipedia.org/wiki/Banana\">Banana\u003C/a> → \u003Ca href=\"https://en.wikipedia.org/wiki/Fruit\">Fruit\u003C/a> → \u003Ca href=\"https://en.wikipedia.org/wiki/Botany\">Botany\u003C/a> → \u003Ca href=\"https://en.wikipedia.org/wiki/Natural_science\">Natural Science\u003C/a> → \u003Ca href=\"https://en.wikipedia.org/wiki/Empirical_evidence\">Empirical evidence\u003C/a> → \u003Ca href=\"https://en.wikipedia.org/wiki/Evidence\">Evidence\u003C/a> → \u003Ca href=\"https://en.wikipedia.org/wiki/Proposition\">Proposition\u003C/a> → \u003Ca href=\"https://en.wikipedia.org/wiki/Meaning_(philosophy)\">Meaning (philosophy)\u003C/a> → \u003Ca href=\"https://en.wikipedia.org/wiki/Philosophy_of_language\">Philosophy of language\u003C/a> → \u003Ca href=\"https://en.wikipedia.org/wiki/Philosophy\">Philosophy\u003C/a>. This characteristic is what I want to graph.\u003C/p>\n\u003Cp>3.) I will use whatever tools already exist for this, this means Python. I have been working on a project which is most likely dead in the water for the past seven months where I have written display drivers, interrupt handlers, and designed completely bespoke PCBs and housings. For a lot of reasons I don’t think I will make a dedicated article or continue working on it (a lot of the reason this died has to do with the price of components skyrocketing after tariffs + tax, I cannot afford to actually make it), but that doesn’t stop the fact that I am tired of writing everything without support. Python has packages called \u003Ca href=\"https://pypi.org/project/mwxml/\">mwxml\u003C/a> and \u003Ca href=\"https://pypi.org/project/mwparserfromhell/\">mwparserfromhell\u003C/a> which are designed specifically to work with Wikipedia pages and parse them into readable text. I am going to use these libraries and some others.\u003C/p>\n\u003Cp>With that out of the way, \u003Ca href=\"https://github.com/TrojanPinata/wikipedia-graphing\">I wrote a program\u003C/a> which first goes through the entire dump and creates a database with all titles, page ids, and a index value for each individual page. This SQLite database ends up being ~2.1GB and is done separately from the rest of the processing in order to do some preliminary removals and reduce continuous runtime when doing the link gathering part. This database when loaded into memory is the title map.\u003C/p>\n\u003Cp>As you can probably tell by the size of just the titles, there is a lot of data here. That’s mostly because of the titles being strings, but that doesn’t take away from the fact that there are \u003Ca href=\"https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia\">7,120,072 pages on the English Wikipedia\u003C/a>, 18 Million if including redirects, disambiguations, and categories. Because of that, something I though a lot about before even starting this project was a method of storing all of the relationships between pages. \u003C/p>\n\u003Cp>Initially, before I decided on the rules of this project, I thought I would be able to show the relationships of all links on all pages. I designed edge lists to do this, but only once my wax wings melted and I fell back to earth did I realize that a simple list containing an entry&#39;s index next to their first link would be way more efficient and only need ~41MB of space in binary, or 23MB when converted to CSR and formatted for Gephi. (Before realizing this was impossible with that much data, my lists were 20GB and had to be constantly written to disk to avoid over saturating memory. Every run also took 13 hours with 4 threads – each thread using ~4GB).\u003C/p>\n\u003Cp>The process for actually collecting the links is far from simple, but I will explain some errors I made to help anyone who wants to do this themselves. My process for going through each page and finding the first link is a single distribution function which creates worker processes which then find the first link of the pages they are given and returns that information, sends it to a buffer, and continues. The title map described above (the SQLite database with the titles and indexes) is stored as a global in memory, this uses more memory, but significantly speeds up the program. The hit to runtime is the use of mwparserfromhell. It is really accurate and does a excellent job, but takes a million years to process everything. If you don’t need accuracy and want to spend the time debugging, just use some regex to find links. This method had some issues, so I just sucked it up and ran it while I was doing other things, but it can get the runtime down from 3 hours to 20 minutes. The last thing I will say is that the \u003Ca href=\"https://gexf.net/\">GEXF\u003C/a> format is kind of weird, and I just opened a GEXF file in a text editor and copied it to allow my program to export in a Gephi readable format. This took me like three days to get right but is super nice because the output file can just be imported into Gephi with titles and indexes embedded, making the workflow much faster when actually making the graph.\u003C/p>\n\u003Cp>All that said, once everything is run and the file is imported, I ran \u003Ca href=\"https://github.com/bhargavchippada/forceatlas2\">ForcedAtlas2\u003C/a> and a Modularity check on the data to spread it out and applied some coloring and titles, and this is what I got.\u003C/p>\n\u003Cimg src=\"https://i.imgur.com/6HtLv2y.png\" alt=\"entire graph\" />\n\n\u003Cp>One thing to note is that this is only 240k nodes instead of millions. \u003Ca href=\"https://www.techradar.com/pro/2026-could-well-be-the-year-of-the-usd500-32gb-ddr5-memory-module-experts-predict-ddr-will-go-up-by-60-percent-in-q1-2026-alone\">That is because of memory, or the lack there of\u003C/a>. The biggest limitation and thing I have learned about this project is how limiting 32GB of memory is when the dataset is so large, and how to manage it all in order to actually show this data. I have never been so memory limited on any project before, and this forced me to think outside the box and writeback whenever possible (another reason why I vouch for fast SSDs). I should also note that Gephi will crash if given too much data, and while you can give it more memory through JVM, even 24GB is too little and will crash if there are too many edges and nodes. Also, with more nodes, your graph will look bad. From my experimenting, the more nodes you have, the more opaque the graph gets from the edges and nodes overlapping in the confined space, and the data means less. 120 thousand nodes is probably the sweet spot, and I added a simple filter to only bring the most connected pages to make the data somewhat interesting.\u003C/p>\n\u003Cp>Actually analyzing the data gave me some results I was not expecting. First thing I noticed is that Philosophy is not in the presentation dataset. Sad, but it makes sense once you consider that Philosophy itself only has a few connections, but it’s child nodes have many. When sorted by number of connections, it does not rank that high. What ranks extremely high, are the pages for \u003Ca href=\"https://en.wikipedia.org/wiki/Town\">Town\u003C/a>, \u003Ca href=\"https://en.wikipedia.org/wiki/City\">City\u003C/a>, \u003Ca href=\"https://en.wikipedia.org/wiki/County\">County\u003C/a>, \u003Ca href=\"https://en.wikipedia.org/wiki/Communes_of_France\">Communes of France\u003C/a>, etc. These location based nodes had thousands of relating nodes, and create some of the largest hubs on the graph. Another observation is that a lot of smaller groups get pushed to the border. This is a limitation of Gephi, but shows how many smaller communities get lost in comparison to these larger communities and relationships. I call these relationships because I found that the first link of each page had a surprisingly consistent level of accuracy for what group a page was part of. You would expect some weird connections when only looking at the first link and ignoring the entire page, but this one link is enough to keep 99.99% of pages in a similar topic cluster or community.\u003C/p>\n\u003Cimg src=\"https://i.imgur.com/OIznSvY.png\" alt=\"community focused on 'City'\" />\n\n\u003Cimg src=\"https://i.imgur.com/PzIhpJs.png\" alt=\"community focused on 'List of sovereign states' and other nodes on border\" />\n\n\u003Cp>This project was fun. I don’t typically do things like this, but between this and the \u003Ca href=\"https://brianchill.us/projects/Zipf\">Zipf’s law article I made\u003C/a>, I’ve started to find fun in doing these break projects which simply demonstrate data and don’t have to do with my direct interests. I feel like they expand my interests and abilities in a way that makes me a more diverse programmer and give me more experience in a wider array of topics. Hopefully you enjoyed this experiment, and if you have more skill than me in this field, I would love to see how you do this or what you come up with.\u003C/p>\n\u003Cp>Until next time.\u003C/p>\n"],"uses":{"params":["slug"]}}]}
